
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
\usepackage[]{algorithm2e}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{data-set}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{SMOTE: Synthetic Minority Over-sampling Technique}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Siddhant~Prakash,~\IEEEmembership{1211092724}
        and~Kunal~Bansal,~\IEEEmembership{1211213169}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem S. Prakash and K. Bansal are with
School of Computing, Informatics and Design Systems Engineering, Arizona State University, Arizona,
AZ, 85281.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: \{sprakas9, kbansal3\}@asu.edu}}
%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space



% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Fundamentals of Statistical Learning, Major Project Final Report, December 2017}%
{Prakash \MakeLowercase{\textit{et al.}}: SMOTE: Synthetic Minority Over-sampling Technique}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
%The abstract goes here.
%\centering
Class imbalance in datasets has been one of the greatest challenge in classification tasks in machine learning.
Often times the most important samples are the ones which is least represented.
This results in an inherent bias towards the majority class while learning the classifier, resulting in a higher rate of mis-classification of minority class samples to majority class.
Our work provides a study of the class imbalance in datasets and reviews ways in which the problem has been dealt with.
Furthermore, we implement an algorithm ``SMOTE", which tackles this problem by generating synthetic samples from the minority class samples and augmenting it to the dataset.
We plot the ROC Curve for classification using Decision Trees, Naive Byes and k-Nearest Neighbor classifiers and compare the performance of our algorithm with the help of ROC Convex Hull (ROCCH) and Area-Under-the-Curve (AUC) metrics.  
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Imbalanced class datasets, minority over-sampling, ROC, Convex Hull, AUC
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{T}{he} seminal work, ``SMOTE: synthetic minority over-sampling technique"\cite{smote} deals with the fundamental  problem of imbalanced class data-sets in machine learning, where, data points of one class are in majority over data points of another class. A number of times the data points belonging to the minority class are more important than the majority class. This majority-minority skew in the data-set leads to classification in favor of majority class samples when one uses standard classification techniques like Naive Bayes or Decision Trees. In several cases, the penalty for mis-classifying these minority classes are much higher than mis-classifying the majority class. For example, the problem of classifying images of mammograms for cancerous cell detection is very sensitive and mis-classification may lead to disastrous outcomes. Still, the number of positive class samples is way outnumbered by the number of negative class samples with the majority (negative) class sample constituting 98\% of total samples.

The authors in \cite{smote} propose a new algorithm to augment the minority samples in a data-set by creating synthetic data points for minority class in the feature space, to even the data distribution between majority and minority classes. They compare the result obtained by this new technique with the results of plain under-sampling the majority samples, as well as over-sampling the minority samples as previously done in other works dealing with the problem.  The results show that their method leads to learning of better classifiers. Further, they show that the classifiers improve with near equal representation from all class in training data. The authors use Receiver Operating Characteristics (ROC) curves as their performance measure. ROC curves provide trade-off between true positive (TP) vs. false positive (FP) which is much more suited to the class imbalance problem than the error rate (accuracy) metric. Area Under the Curve (AUC) and convex hull of ROC curve are used for the experiments as they provide good comparison of the classifiers' performance in class imbalance scenarios both quantitative and qualitatively. 

% Many previous work tries to tackle the problem of imbalanced datasets in broadly two ways.\\
% First, to assign distinct penalty for training data and \\
% Second, to change the dataset by either under-sampling the majority class or over-sampling the minority class.\\
% The authors approach mixes the two and uses a unique algorithm to over-sample the minority class.\\
% They show there performance using the AUC of ROC curve and ROC convex hull method.\\
% They compare there classification for C4.5 Decision Tree, Ripper and Naive Bayes classifiers. \\

%Summarize the tasks
In our project, we learn to deal with the challenges of class imbalanced data-sets and how to overcome them. We reproduce the work done in \cite{smote} and understand the problem by going over the previous works cited in this paper. Additionally, we summarize the various techniques researchers came up to deal with this issue in Section \ref{sec:works}. We overview the performance metrics used in this paper and explain why the authors chose to use these metrics, as well as how are they relevant to this particular problem in Section \ref{sec:metrics}. We then move on to the implementation of the algorithm SMOTE proposed in the paper, along with two more approaches which was used to emphasize the novelty of this technique in Section \ref{sec:implement}. We replicate the experiments section on 3 of the 9 datasets listed in the paper, using Decision Trees, Nearest Neighbours and Naive Bayes classifiers to learn classification models in Section \ref{sec:exp}. We also use the prediction results from these models to plot ROC curves, and taking AUC and convex hull of the curve as evaluation metric, compare our plots with the plots obtained in the paper and provide a detailed discussion on the results in Section \ref{sec:discuss}. Finally, we conclude our work with an overview of future works in Section \ref{sec:conclude}. 

%\subsection{Subsection Heading Here}
%Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Previous Works}
\label{sec:works}
Most of the cases of imbalanced dataset is dealt in two ways, viz. under-sampling the majority class samples or over-sampling the minority class samples.
Different domain requires different techniques for the same based on their requirement. 
When it comes to under-sampling the majority class, Kubat et al \cite{kubat97}\cite{kubat98} experimented with the same.
In Kubat and Matwin \cite{kubat97}, the majority class is selectively under sampled while the minority class sampling remains fixed. 
The performance metric used for the classifier is geometric mean which is not as expressive as a ROC curve and corresponds to just one point on it.
Related to the above, the SHRINK system of Kubat et al \cite{kubat98} classified the overlapping reasons of both majority and minority classes as positive, leading to a "best positive region" classification.
 
Another study on under-sampling of dataset was performed by Japkowicz et al \cite{japkowicz00}. 
In her study, she explored different sampling techniques on artificial 1D data for better evaluation of concept complexity.
Her exploration involved under-sampling as well as resampling of data.
Both strategies involved two different methods, viz. random and focused. 
Random resampling used samples from minor class to be sampled randomly until they matched major class samples, while focused resampling used only the boundary points between minor and major class.
In random under-sampling, the samples from majority class were removed randomly to match the minority class samples, in contrast to focused under-sampling which under sampled majority class samples lying further away.
Her study revealed the efficacy of both the sampling techniques but did not provide any clear advantage in the domain considered.

While under sampling approach works, other works uses under-sampling of majority class samples along with over-sampling of minority class samples for learning a better classifier.
Ling and Li et al \cite{ll98} uses lift analysis to measure classifier's performance in the domain of marketing analysis problem.
They ranked the test examples by confidence measures and used lift as the evaluation criteria.
In one of the experiments they performed, they under sampled the majority class and observed that the best lift index is obtained when there is equal representation of the classes.
In another experiment, they over-sampled the minority samples with replacement to match the negative samples but could not prove the same as significant. 
The work present in this paper is similar in strategy, but the over-sampling techniques is different.
Another work which uses the idea of under-sampling as well as over-sampling of data to overcome class imbalance problem is Solberg and Solberg et al \cite{solberg96} .
They use SAR imagery dataset obtained for classification of oil slicks which is heavily biased towards look-alike data compared to oil slicks (98\%-2\%). 
They created a new dataset by over-sampling the oil slicks data randomly and under-sampling the look-alike data to create equal class distribution.
As a result, on learning a classification tree on the balanced dataset they obtained better error rates on both classes compared to training on imbalanced dataset.
Domingos et al \cite{domingos99} also take the same approach to deal with class imbalance by introducing a "metacost" term to under-sampling as well as over-sampling.
The work shows the metacost improves over either, and proves that under-sampling of majority class does better than over-sampling of minority class.

Other researchers (DeRouin et al \cite{derouin91}) tried to use the same on feed-forward neural networks which is not able to learn to discriminate between classes sufficiently due to the same class imbalance problem.
The learning rate of the neural network was adapted according to the class distribution in the data set.
Experimenting over artificial as well as real-world training data with multi-class problem provided better classification accuracy for minority class.
In information retrieval domain, document classification is one of the challenging problems which is affected by this class imbalance.
Creating  a simple bag-of-words model results in interesting words samples as a minority due to very limited instances of such words in the document.
Thus, in IR domain, the performance metric is replaced from error rates and instead, precision and recall terms are used for performance measurements.
$$recall = \frac{TP}{TP + FN}$$ $$precision = \frac{TP}{TP + FP}$$
We will see what we mean by the terms True Positive (TP), False Positive (FP) and False Negative (FN) in the next section.
In the same domain, Mladenic and Grobelnik \cite{mg99} proposed a feature subset selection approach to deal with class imbalance.
They found out that using odd ratio along with Naive Bayes classifier performs best in the domain. 
Odds ratio incorporates target class information giving better result over information gain which is computed per word for each class.
In \cite{pf98}, Provost and Fawcett introduced the ROC convex hull method for performance evaluation of the classifier in which ROC space is used to separate classification performance from class and cost distribution. 


\section{Evaluation Criteria and Performance Metrics}
\label{sec:metrics}
\begin{figure}[!b]
	\centering
	\includegraphics[width=2.5in]{code/figures/mammography_ROC}
	\caption{Example of an ROC Curve}
	\label{fig:roc}
\end{figure}



\begin{table}[!t]
    \centering
    \begin{tabular}{|c||c|c|}
    	\hline
         & Predicted Negative & Predicted Positive  \\ \hline \hline  
        Actual Negative & True Negative (TN) & False Positive (FP) \\ \hline
        Actual Positive & False Negative (FN) & True Positive (TP) \\ \hline 
    \end{tabular}
    \caption{Confusion Matrix}
    \label{tab:confmat}
\end{table}

Confusion matrix, shown in Table \ref{tab:confmat}, is one of the most common method in machine learning used to evaluate performance of a (2-class) classification problem for imbalanced class datasets.
As shown in the table, the columns are predicted class and the rows are actual class. 
Over a dataset of finite samples, the count of correctly classified negative samples is termed as True Negative (TN), while the count of incorrectly classified negative samples is termed as False Positive (FP).
Similarly, the count of incorrectly classified positive sample is termed as False Negative (FN), while the count of correctly classified positive sample is termed as True Positive (TP).


For any classification task, predictive accuracy is defined as the total number of correctly classified samples over total number of samples. 
Mathematically, it is given by, $$ Accuracy = \frac{TP + TN}{TN + FP + FN + TP}$$
In machine learning, we evaluate the performance of a classifier by its error rate which is given by, $$Error Rate = 1 - Accuracy$$
This performance measure works well for balanced class data sets, but for imbalanced datasets a much wider used metric is the Receiver Operating Characteristics (ROC) curves. 

A typically ROC curve is a plot of percentage True Positive vs percentage False Positive. 
One such curve is shown in Figure \ref{fig:roc}.
We have \%ge FP on the X-axis given by, $$\%ge~FP = \frac{FP}{TN + FP}$$
and \%ge TP on the Y-axis given by, $$\%ge~TP = \frac{TP}{TP + FN}$$
As evident from the definition, the ideal point on the curve will be (0, 100), signifying that all positive examples are classified correctly while no negative samples are mis-classified as positive.
The Area Under the Curve (AUC) can be taken as a good metric for comparing different classifiers, but these can be suboptimal for some specific cost and class distributions.
Thus, the convex hull of ROC curve, being potentially optimal, is also taken as one of the performance metrics.



\section{Implementation}
\label{sec:implement}
In this section we provide the details of the various approaches we implemented from the paper.
Using the ``mammography" dataset\cite{mammo}, we show the effectiveness of SMOTE in this class imbalanced scenario.
The dataset consist of 11,183 samples of which 10,923 are negative samples while we have only 260 positive samples.
The dataset has 6 attributes, while we learn the classifier on 2 attributes, by decomposing the dataset using PCA, for better visualization. 
In Figure \ref{fig:compare}, we show the decision boundaries in the feature space, created by classifying 10\% of the total data containing \textbf{18} positive samples and \textbf{1,100} negative samples.
We have used the classifiers implemented in Scikit Learn library\cite{sklearn} with codes written in Python 2.7.1.
The codes and dataset are provided in the supplementary material.

\subsection{Over-sampling with replacement}
Learning a classifier on the imabalanced class data gives us a biased classification towards the majority class samples (red circles).
From Figure \ref{fig:a}, we observe that the classification decision boundary using the original data is not very accurate. 
Many minority class samples (blue circles) are classified under majority class (red region) resulting in decrease in true positive rate. 
The decision boundary is more general and spread away from minority (positive) class samples too.

One of the earliest ways suggested by researchers to overcome this issue is over-sampling the minority class samples, in order to reduce the bias towards the majority class.
We implement the over-sampling approach by duplicating the minority class samples.
The degree of over-sampling in our implementation is 500\%, i.e. if the dataset had 18 positive class samples and 1,100 negative class samples, as is the case in Figure \ref{fig:compare}, we upsample the positive class sample to 90.
%We do so by appending already present positive class samples in the dataset 4 times to the complete dataset.
As a result, we see the decision boundary for minority class samples (blue region) shrink to enclose each individual positive sample.
In other words, the data starts overfitting towards the positive class samples as more and more leaf nodes are added to the decision tree leading to very specific decision boundaries in the feature space.
We see the same happening in Figure \ref{fig:b} with the 6 positive samples in the region around (0, 5).

\begin{figure}[!t]
\centering
\subfloat[Case I]{
	\includegraphics[width=1.5in]{code/figures/a}%
	\label{fig:a}
	}
\hfil
\subfloat[Case II]{
	\includegraphics[width=1.5in]{code/figures/b}%
	\label{fig:b}
	}
\hfil
\subfloat[Case III]{
	\includegraphics[width=1.5in]{code/figures/c}%
	\label{fig:c}}
\caption{Comparison of classification using Decision Trees on (a) Original dataset (b) Minority Over-sampling with replacement dataset, and (c) Over-sampling using SMOTE 500\% dataset.}
\label{fig:compare}
\end{figure}

\subsection{SMOTE}
Over-sampling of minority samples provides better accuracy but increases the complexity of classifier learnt manifolds. 
Moreover, the results does not generalize well as the decision region become very specific to the few minority samples present in the feature space leading to increase in false negatives.
To overcome these issues a new over-sampling technique is proposed which augments the minority class samples by adding ``synthetic" data.
The algorithm works in the feature space of minority data samples.
For a given degree of over-sampling rate, say 200\%, the algorithm finds k-nearest neighbors of a minority sample and generates 2 random points on the line joining any 2 of the k-nearest neighbors found.
We use nearest neighbors class implementation from scikit-learn library to find the nearest neighbor of each data point in the minority feature space.
Algorithm \ref{algo:smote} gives the steps to generate ``SMOTED" data.

Figure \ref{fig:c} compares the effect of over-sampled data using SMOTE by 500\%, with original data (Figure \ref{fig:a}) and replaced data (Figure \ref{fig:b}).
It is clear from the plot that synthetic data obtained using SMOTE helps classifier to generalize well over minority samples by providing better synthetic data to overcome the class imbalance.
The decision region becomes larger and less specific as is the case with over-sampling by replacement, at the same time reducing mis-classification by creating better decision boundary which is seen in the case of classification using original data.


\subsection{SMOTE with under-sampling}
\label{subsec:smoteu}
Over-sampling minority class samples with the help of SMOTE helps in increasing the percentage of minority samples, but still doesn't help overcoming the effect of grossly skewed data perfectly.
To overcome this bias towards majority class sample, we under-sample the majority samples in addition to SMOTing the dataset.
The under-sampling is done to a degree with respect to the number of minority class samples.
We first SMOTE the dataset and obtain the final number of minority class samples and then under-sample the majority class sample to get the desired degree of under-sampling.
For example, in the above example, we have 18 minority samples and 1,100 majority samples. 
SMOTing the dataset by 500\% results in an increase in minority samples to 90.
Now, under-sampling the dataset by a degree of 200\% means drawing out \textit{half the samples of minority class samples, at random, from the total majority class samples}, i.e using 45 samples from the 1,100 majority (negative) class samples along with the 90 minority (positive) class samples to train the classifier.

\begin{algorithm}[!h]
 \KwData{\#Minority Samples T; Degree of SMOTE N\%; \#nearest neighbor k}
 \KwResult{synthetic[][] := (N/100)*T synthetic minority samples }
 initialization:= original[][]; nIdx=0;  nAttr\;
 \If{N $<$ 100}{
   T = (N/100) * T\;
   N = 100\;
   }
   N = (int) (N/100)\;
 \For{i := 1 to T}{
  nnArray := compute k nearest neighbor\;
  \While{N != 0}{
  nn := random(1, k)\;
  \For{attr := 1 to nAttr}{
  dif = original[nnArray[nn]][attr] - sample[i][attr]\;
  gap = random(0, 1)\;
  synthetic[nIdx][attr] = sample[i][attr] + gap*dif\;
  }
  nIdx++\;
  N-=1\;
  }
 }
 \caption{Steps to perform over-sampling using SMOTE to generate synthetic minority data}
 \label{algo:smote}
\end{algorithm}


\begin{table}[!t]
    \centering
    \begin{tabular}{|c||c|c|}
    	\hline
         Datasets & \#Minority (+ve) Class & \#Majority (-ve) Class  \\ \hline \hline  
        Mammography & 260 & 10,923 \\ \hline
        SatImage & 626 & 5,809 \\ \hline 
        Pima Indian & 268 & 500 \\ \hline
    \end{tabular}
    \caption{Class Distribution in Datasets Used}
    \label{tab:dataset}
\end{table}

\begin{table*}[t]
    \centering
    \begin{tabular}{|c||c|c|c|c|c|c|}
    	\hline
         Datasets & Under & SMOTE 100 & SMOTE 200 & SMOTE 300 & SMOTE 400 & SMOTE 500  \\ \hline \hline  
        Mammography &  0.7220 & 0.7290 & 0.7776 & \textbf{0.7919} & 0.7918 & 0.7752 \\ \hline
        SatImage & 0.7866 & 0.7881 & 0.8259 & 0.8318 & \textbf{0.8380} & 0.8283 \\ \hline 
        Pima Indian & 0.6581 & 0.6567 & 0.7014 &  0.7145 & 0.7207 & \textbf{0.7327} \\ \hline
    \end{tabular}
    \caption{AUC of the ROC Curve for plain under-sampling and varying degree of minority over-sampling using SMOTE (100\%, 200\%, 300\%, 400\% \& 500\%)}
    \label{tab:auc}
\end{table*}


\section{Experiments}
\label{sec:exp}
We experiment with our implementation of plain under-sampling and under-sampling with SMOTE under varying degrees of under-sampling and SMOTE combinations.   
Our aim is to tackle the class imbalance problems in datasets, thus, we test the algorithm on 3 different dataset with different class distributions.
We use Decision Trees, Naive Bayes and k-NN classifiers with varying under-sampling and SMOTE degrees to (i) analyze the effectiveness of SMOTing data, and (ii) compare the classifiers learning the best decision boundaries.
We plot the ROC curves and use ROC Convex Hull (ROCCH) and Area Under the Curve (AUC) as our metrics to draw our conclusions.
\subsection{Datasets}
The 3 dataset we use are Mammography Dataset \cite{mammo}, Pima Indian dataset \cite{pima} and SatImage dataset \cite{satimg}.
The class distribution of the datasets are provided in Table \ref{tab:dataset}.
We have provided the codes to convert raw data file into comma separated file format (CSV), which is the input dataset file format in our implementation and experiments code, in the supplementary material. 

Each datasets are multi-dimensional, with mammography dataset consisting 6 attributes, Pima Indian having 8 attributes and SatImage datasets having 36 attributes.
Wherever needed, we reduce the dimensionality of dataset using PCA for preserving maximum information, faster computation and visualization of decision boundary in 2-D feature space.
\begin{figure}[!b]
\centering
\subfloat[Plot I]{
	\includegraphics[width=1.5in]{code/figures/mammography_ROC1}%
	\label{fig:mammo1}
	}
\hfil
\subfloat[Plot II]{
	\includegraphics[width=1.5in]{code/figures/mammography_ROC2}%
	\label{fig:mammo2}
	}
\hfil
\caption{ROC Curve plots for Mammography dataset.}
\label{fig:mammo}
\end{figure}

\subsection{Classifiers}
Our main classifier for testing the algorithm implementation is \textbf{Decision Trees}.
In the paper, the authors use C4.5 variant of Decision Trees to test the classification quality.
We have used the implementation of Decision Trees provided by scikit-learn library, which is an optimized version of C4.5 algorithm called Classification and Regression Trees (CART) algorithm.
As stated in the documentation \cite{dtree}, ``CART is very similar to C4.5, but it differs in that it supports numerical target and does not compute rule sets. 
CART constructs binary trees using the feature and threshold that yield the largest information gain at each node."

In addition to decision tree classifiers, we compare the performance of the same algorithms on \textbf{Naive Bayes} classifier with varying minority class priors.
The paper also uses Ripper classifier to compare the performance, but we failed to locate any existing implementation of Ripper classifier in a 3rd party library in Python.
Instead we use \textbf{k-Nearest Neighbor} classifier with a fixed value of k for comparison with Decision Tree classifier, and varying values of k for comparison amongst k-NN classifiers.
Our change in classifier is guided by the intuition that the algorithm augments data in the feature space, and since k-NN classification works directly in the feature space, it should provide a better classification accuracy.
The value of k in all k-NN a implementation is 5 for all our experiments.

\subsection{ROC Curves and Cross-Validation}
In order to evaluate the performance we plot the Receiver Operating Characteristics (ROC) curves for varying parameters of classification.
ROC curves are essentially a plot of False Positive Rate (FPR) on x-axis vs True Positive Rate (TPR) on y-axis which measures a classifier's performance in more detail than a simple accuracy measure.
This granularity is of great use in a class imbalance scenario because it gives an idea about the degree of mis-classification in both positive as well as negative class.  
Ideally, we will like a low False Positive Rate and a high True Positive Rate, resulting in a shift in ROC curve towards the (0, 100) point. 
As the curve goes nearer to this point, the classification gets better.
\begin{figure}[!b]
\centering
\subfloat[Plot I]{
	\includegraphics[width=1.5in]{code/figures/pimaindians_ROC1}%
	\label{fig:pima1}
	}
\hfil
\subfloat[Plot II]{
	\includegraphics[width=1.5in]{code/figures/pimaindians_ROC2}%
	\label{fig:pima2}
	}
\hfil
\caption{ROC Curve plots for Pima Indians dataset.}
\label{fig:pima}
\end{figure}

In our experiments we draw two plots for each dataset. 
The first plot contains Decision Tree as the base classifier and we compare the Decision Tree implementation of plain under-sampling of majority class samples at rates varying from \textbf{10\%} to \textbf{2000\%} as described in Section \ref{subsec:smoteu}. 
For plain under-sampling we do not SMOTE the dataset before under-sampling.
We compare the ROC curve obtained by plain under-sampling with that obtained by under-sampling with SMOTE.
To obtain the second ROC curve, we over-sample the dataset with varying degrees from \textbf{100\%} to \textbf{500\%}, based on the dataset class distributions.
In the same plot we compare Decision Tree classifier with Naive Bayes classifier, by adding another ROC curve plotted using classification from Naive Bayes with varying class priors of minority class.
In the implementation provided by scikit-learn, the class priors are automatically calculated as the frequency of each class samples, as stated in the documentation\cite{nbayes}.
Thus, we feed in the data used in plain under-sampling to NB classifier as it varies the ratio between the two class samples. 
The various class priors used to plot the curve vary for different dataset based on the class distribution ratio of the dataset.
We report the class priors at the end, after performing each experiment.
In the second plot we replace decision tree classifier with k-NN classifier as the base classifier. 
We provide a comparison of the classifier with respect to plain under-sampled data and under-sampled data with SMOTE as with previous case.

Each point on the ROC curve is obtained by classifying the training data using a \textbf{10-fold cross validation} .
In an n-fold cross validation, each dataset is divided into n equal parts randomly, and (n-1) of those parts are use for training while 1 left-out part is used for validation to improve the classifier parameters.
This process is iteratively performed n times and classification metric is generated with each prediction.
The points are the mean False Positive Rate vs mean True Positive Rate for the 10 different classification with a particular configuration of over-sampling/ under-sampling combination.
We followed the ROC curve implementation with cross-validation of scikit-learn library\cite{roc}, with modification to plot curves as we needed for our particular experiments.
Figure \ref{fig:mammo}, \ref{fig:pima} and \ref{fig:sat} provides the ROC curves we plot with comparison on all 3 datasets and the 3 classifiers we use. 
The average time taken to plot the ROC curves for Mammography dataset is \textbf{91.9 secs}, for SatImage dataset is \textbf{297.3 secs} and for Pima Indians dataset is \textbf{68.5 secs}.

\begin{figure}[!b]
\centering
\subfloat[Plot I]{
	\includegraphics[width=1.5in]{code/figures/sat_ROC1}%
	\label{fig:sat1}
	}
\hfil
\subfloat[Plot II]{
	\includegraphics[width=1.5in]{code/figures/sat_ROC2}%
	\label{fig:sat2}
	}
\hfil
\caption{ROC Curve plots for SatImage dataset.}
\label{fig:sat}
\end{figure}



\subsection{Convex Hull and AUC}

The performance of the ROC curve is quantified using ROC Convex Hull and AUC metric.
Convex Hull gives the boundary of a potential optimal classifier in comparison of multiple ROC curves.
ROC Curves themselves may not be optimal at every instance. 
Thus, we use the hull to find out when a given classifier/method is optimal and when it can be replaced.
We have implemented convex hull in all our plots, which should be taken as the perfect classification ROC curve.
We construct the hull by first creating an array of all the 2D points on the ROC Curves using all classifier/method combination.
Then we call the ConvexHull implementation of Scipy library \cite{scipy} provided in \textit{scipy.spatial} class, which essentially uses the QuickHull algorithm of \cite{qhull}\cite{pf01}.
This breaks the decision space into multiple simplices of the convex hull and finally we draw each individual simplex to complete the convex hull of the curves.

Area Under-the-Curve (AUC) is another metric which we use to evaluate the performance of each classification.
In simple terms, the more the value of AUC, the better the classification as the curve will be nearer to the ideal point (0, 100) to maximize the area.
We again use the implementation provided in scikit-learn library to calculate the AUC for each ROC curve.
We summarize the average AUC values obtained from 10 runs of the experiment in Table \ref{tab:auc}.
The average time taken to calculate the AUC for Mammography dataset is \textbf{17.4 secs}, for SatImage dataset is \textbf{88.8 secs} and for Pima Indians dataset is \textbf{17.2 secs}.


\section{Discussions}
\label{sec:discuss}
In Figure \ref{fig:mammo1}, \ref{fig:pima1} and \ref{fig:sat1} , we plot the results obtained by SMOTE based minority oversampling with majority under-sampling technique, previous techniques of plain majority under-sampling learnt on Decision Trees and changing minority class priors in Naive Bayes.
%First, we try to compare the smote based sampling at different levels of majority under-sampling with the Naive Bayes and plain under-sampling methods. 
For mammography dataset, we observe that we obtain optimal results at 400\% SMOTE oversampling with varying under-sampling degree. 
With SMOTE, we obtained better results than Naive Bayes or plain under-sampling which is evident from the convex hull, as most of the points from SMOTE lie on the convex hull.
For Pima Indians dataset,  we observe that both Naive Bayes with different minority class priors and SMOTE works equally well where each lies partially on the convex hull but even though SMOTE is better because more points on the convex hull comes from SMOTE than Naive Bayes. 
For SatImage Dataset, Naive Bayes performs better than SMOTE which can be attributed to the higher number of attributes in SatImage.
In all the datasets, we observe SMOTE with under-sampling performs way better than plain under-sampling with Decision Trees.

Next in Figure \ref{fig:mammo2}, \ref{fig:pima2} and \ref{fig:sat2}, we use nearest neighbor classifier with the same SMOTE with under-sampling and plain under-sampling. 
Here, we observe that for all three datasets, SMOTE based classifier performed better than plain under-sampling which is in line with our previos results with different classifiers. 
Most of the points on the convex hull comes from SMOTEd dataset which proves SMOTE classification with SMOTE is most optimal.

Last, we compare Area Under Curve for different degree of SMOTE i.e. 100\%, 200\%, 300\%, 400\% and 500\% for all the three datasets with plain under-sampling on mammography dataset trained with Decision Tree classifier. 
We again observe that SMOTE perform better than under-sampling though at different degrees of minority oversampling. 
As we can observe from the values, for mammography SMOTing at 300\% is the best, for SatImage SMOTing at 400\% is best while for Pima Indians SMOTing at 500\% is best with maximum area under their respective ROC curves.

These results confirm our intuition that SMOTE with under-sampling perform much better than plain under-sampling.
This can be attributed to the fact since we are increasing the minority class samples, the training set has more data to learn better classifier with better class distributions.
Again, this over-sampling method is better than replacement because new data points are generated in the feature space, which will again help the classifier generalize well over the dataset.
While we can not argue that augmenting the data with SMOTE helps, the degree of SMOTE remains a variable in the implementation.
If we specify a lower degree the number of synthetic samples created do not boost the minority class ratio much while if we use very high degree, the high number of minority class sample will start over-fitting the data similar to what we see in over-sampling with replacement.
Thus, finding the degree based on the original class distributions such that SMOTing negates the effect of dis-proportionate distribution on learning classifier is a challenge with this approach.

\section{Conclusion \& Future Scope}
\label{sec:conclude}
Through this project, we have studied the problem of class imbalance in dataset. 
We reviewed techniques to overcome this challenge, and went one step ahead by implementing one of the popular techniques used to overcome this problem, SMOTE.
We compared our implementation of synthetic over-sampling with over-sampling with replacement and showed that the classifier learnt by our method is more general.
Through our experiments, we show that while SMOTE with under-sampling works best for specific classifiers like decision trees and nearest neighbor, comparing classifiers' performance, naive bayes with varying minority class samples can also be used for better classification.
While convex hull of ROC curves show that SMOTE is optimal for classification, AUC shows that varying degree of SMOTE helps in improving the classifier further. 

Thus, as future works that should be a natural extension of this algorithm is to automatically find the degree of SMOTE to be used for augmenting the minority class sample.
The degree should be neither high nor low as at low degrees the algorithm will not be affective, while at higher degree will lead to generation of data specific to training set.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Appendix Section 1}
%Appendix one.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

%The authors would like to thank . . .
The authors would like to thank Professor Baoxin Li and the TAs Vijetha Gattupalli and Kevin Ding for providing their valuable guidance. 
We appreciate their considerate efforts which helped us in completing the project successfully.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{10}

\bibitem{smote}
Chawla, Nitesh V., et al. "SMOTE: synthetic minority over-sampling technique." Journal of artificial intelligence research 16 (2002): 321-357.

\bibitem{sklearn}
Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.

\bibitem{mammo}
Woods, K., Doss, C., Bowyer, K., Solka, J., Priebe, C., \& Kegelmeyer, P. (1993). Compara- tive Evaluation of Pattern Recognition Techniques for Detection of Microcalcifications in Mammography. International Journal of Pattern Recognition and Artificial Intel- ligence, 7(6), 1417–1436.
\url{http://odds.cs.stonybrook.edu/mammography-dataset/}

\bibitem{pima}
Blake, C., \& Merz, C. (1998). UCI Repository of Machine Learning Databases . Department of Information and Computer Sciences, University of California, Irvine.
\url{https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes}

\bibitem{satimg}
Blake, C., \& Merz, C. (1998). UCI Repository of Machine Learning Databases . Department of Information and Computer Sciences, University of California, Irvine.
\url{https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/}

\bibitem{dtree}
Decion Trees, Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
\url{http://scikit-learn.org/stable/modules/tree.html}
\bibitem{nbayes}
Naive Bayes, Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
\url{http://scikit-learn.org/stable/modules/naive_bayes.html}


\bibitem{roc}
ROC Curves, Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
\url{http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py}

\bibitem{scipy}
Scientific Computing Tools for Python. SciPy contributors.
\url{https://www.scipy.org/index.html}
 
\bibitem{qhull}
Barber, C. B., Dobkin, D. P., \& Huhdanpaa, H. (1996). The quickhull algorithm for convex hulls. ACM Transactions on Mathematical Software, 22 (4), 469–483. 
\url{http://www. acm.org/pubs/citations/journals/toms/1996-22-4/p469-barber/}.

\bibitem{pf01}
Provost, F., \& Fawcett, T. (2001). Robust Classification for Imprecise Environments. Ma- chine Learning, 42/3, 203–231.

\bibitem{kubat97}
Kubat, M., \& Matwin, S. (1997). Addressing the Curse of Imbalanced Training Sets: One Sided Selection. In Proceedings of the Fourteenth International Conference on Machine Learning, pp. 179–186 Nashville, Tennesse. Morgan Kaufmann.

\bibitem{kubat98}
Kubat, M., Holte, R., \& Matwin, S. (1998). Machine Learning for the Detection of Oil Spills in Satellite Radar Images. Machine Learning, 30, 195–215.

\bibitem{japkowicz00}
Japkowicz, N. (2000). The Class Imbalance Problem: Significance and Strategies. In Proceedings of the 2000 International Conference on Artificial Intelligence (IC-AI’2000): Special Track on Inductive Learning Las Vegas, Nevada.

\bibitem{ll98}
Ling, C., \& Li, C. (1998). Data Mining for Direct Marketing Problems and Solutions. InProceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98) New York, NY. AAAI Press.

\bibitem{solberg96}
Solberg, A., \& Solberg, R. (1996). A Large-Scale Evaluation of Features for Automatic Detection of Oil Spills in ERS SAR Images. In International Geoscience and Remote Sensing Symposium, pp. 1484–1486 Lincoln, NE.

\bibitem{domingos99}
Domingos, P. (1999). Metacost: A General Method for Making Classifiers Cost-sensitive. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 155–164 San Diego, CA. ACM Press.

\bibitem{derouin91}
DeRouin, E., Brown, J., Fausett, L., \& Schneider, M. (1991). Neural Network Training on Unequally Represented Classes. In Intellligent Engineering Systems Through Artificial Neural Networks, pp. 135–141 New York. ASME Press.

\bibitem{mg99}
Mladeni ́c, D., \& Grobelnik, M. (1999). Feature Selection for Unbalanced Class Distribution and Naive Bayes. In Proceedings of the 16th International Conference on Machine Learning., pp. 258–267. Morgan Kaufmann.

\bibitem{pf98}
Provost, F., Fawcett, T., \& Kohavi, R. (1998). The Case Against Accuracy Estimation for Comparing Induction Algorithms. In Proceedings of the Fifteenth International Conference on Machine Learning, pp. 445–453 Madison, WI. Morgan Kauffmann.

\end{thebibliography}



% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}
%
%% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
%
%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage
%
%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
%
% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


